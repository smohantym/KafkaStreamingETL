from datetime import datetime
import os

import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models.taskinstance import TaskInstance


# Streaming job writes into this directory (mapped from ./data/output on host)
DEFAULT_OUTPUT_PATH = "/opt/airflow/data/parquet"


def prepare_params(**context):
    """
    Prepare parameters for the batch job and push them via XCom.
    """
    ti: TaskInstance = context["ti"]
    run_date = context["ds"]  # Airflow logical run date (YYYY-MM-DD)

    params = {
        "output_path": context["params"].get("output_path", DEFAULT_OUTPUT_PATH),
        "run_date": run_date,
    }

    ti.xcom_push(key="batch_params", value=params)
    return params


def run_batch_job(output_path: str, run_date: str) -> int:
    """
    Batch job using pandas/pyarrow:
    - Reads Parquet generated by Spark streaming
    - Filters by event_date = run_date
    - Aggregates revenue & events by country/segment
    - Adds a rank per country
    - Writes summarized Parquet to a new directory
    """

    if not os.path.exists(output_path):
        raise FileNotFoundError(f"Input path does not exist: {output_path}")

    print(f"[BATCH] Reading Parquet from {output_path}")

    # Pandas can read a directory of Parquet files
    df = pd.read_parquet(output_path)

    if df.empty:
        print("[BATCH] No data found in input Parquet directory.")
        return 0

    # event_date is written by Spark as a date column
    df["event_date"] = pd.to_datetime(df["event_date"]).dt.date
    run_date_dt = datetime.strptime(run_date, "%Y-%m-%d").date()

    df_day = df[df["event_date"] == run_date_dt]

    if df_day.empty:
        print(f"[BATCH] No rows for event_date={run_date_dt}")
        return 0

    # Aggregate to daily metrics
    daily = (
        df_day.groupby(["event_date", "country", "segment"], as_index=False)
        .agg(
            daily_revenue=("total_amount", "sum"),
            daily_events=("unique_events", "sum"),
        )
    )

    # Rank segments within each country by revenue
    daily["revenue_rank_in_country"] = (
        daily.sort_values(["event_date", "country", "daily_revenue"], ascending=[True, True, False])
        .groupby(["event_date", "country"])
        .cumcount()
        + 1
    )

    # Build a tiny dim_users table (same logic as Spark version)
    users_data = [
        (1, "retail", True, "IN"),
        (2, "retail", False, "IN"),
        (3, "enterprise", True, "US"),
        (4, "enterprise", False, "US"),
        (5, "partner", False, "DE"),
    ]
    dim_users = pd.DataFrame(
        users_data, columns=["user_id", "segment", "is_premium", "country"]
    )

    # Join on segment to enrich with premium flag (drop duplicate segments first)
    dim_seg = dim_users[["segment", "is_premium"]].drop_duplicates("segment")
    joined = daily.merge(dim_seg, on="segment", how="left")

    # Summary: premium vs non-premium revenue by country & date
    summary = (
        joined.groupby(["event_date", "country", "is_premium"], as_index=False)
        .agg(total_revenue=("daily_revenue", "sum"))
    )

    target_dir = os.path.join(os.path.dirname(output_path), "daily_segment_metrics")
    os.makedirs(target_dir, exist_ok=True)
    target_path = os.path.join(target_dir, f"metrics_{run_date}.parquet")

    print(f"[BATCH] Writing {len(summary)} rows to {target_path}")
    summary.to_parquet(target_path, index=False)

    return len(summary)


def run_batch_task(**context):
    """
    Airflow task wrapper around run_batch_job.
    """
    ti: TaskInstance = context["ti"]
    params = ti.xcom_pull(task_ids="prepare_params", key="batch_params")

    output_path = params["output_path"]
    run_date = params["run_date"]

    row_count = run_batch_job(output_path=output_path, run_date=run_date)
    ti.xcom_push(key="row_count", value=row_count)
    return row_count


def quality_check(**context):
    """
    Soft quality check: log row_count but don't fail the DAG if it's zero.
    Useful for dev when there may be no data yet.
    """
    ti: TaskInstance = context["ti"]
    row_count = ti.xcom_pull(task_ids="run_batch", key="row_count")

    if row_count is None:
        row_count = ti.xcom_pull(task_ids="run_batch")

    row_count = int(row_count or 0)
    print(f"[QUALITY] Batch row_count = {row_count}")

    if row_count == 0:
        print("[QUALITY] WARNING: No rows produced by batch job (but not failing DAG).")



with DAG(
    dag_id="kafka_spark_etl",
    description="Kafka → Spark Streaming → Parquet → Batch analytics via Airflow",
    schedule_interval="@daily",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    default_args={
        "owner": "airflow",
        "depends_on_past": False,
    },
    params={
        "output_path": DEFAULT_OUTPUT_PATH,
    },
    max_active_runs=1,
    tags=["spark", "kafka", "etl"],
) as dag:

    t_prepare = PythonOperator(
        task_id="prepare_params",
        python_callable=prepare_params,
    )

    t_batch = PythonOperator(
        task_id="run_batch",
        python_callable=run_batch_task,
    )

    t_quality = PythonOperator(
        task_id="quality_check",
        python_callable=quality_check,
    )

    t_prepare >> t_batch >> t_quality
